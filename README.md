# Donut Training/Inference Template

A minimal but complete project to fine-tune and run inference with Naver CLOVA Donut (`naver-clova-ix/donut-base`) using Hugging Face Transformers. Includes data splitting, training, logging, checkpointing, and single-image inference.

## Environment Setup (Conda)

- Create and activate environment
  - `conda create -n donut python=3.10 -y`
  - `conda activate donut`
- Install core deps (choose one)
  - CPU (simple): `pip install torch transformers pillow pandas scikit-learn`
  - GPU (recommended): follow install selector at https://pytorch.org/get-started/locally/ to match your CUDA, then:
    - Example (CUDA 12.1): `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`
    - Then: `pip install transformers pillow pandas scikit-learn`

## Project Structure

```
.
├── config.json                   # Main config: model, training, data paths
├── data
│   ├── original_data
│   │   └── image_data.csv        # Input CSV with columns: image_path,text
│   ├── train_data.csv            # Generated by scripts/divide_data.py
│   ├── validation_data.csv       # Generated by scripts/divide_data.py
│   └── test_data.csv             # Generated by scripts/divide_data.py
├── results/                      # Each run creates a timestamped folder
├── scripts
│   └── divide_data.py            # Split original CSV into train/valid/test
├── src
│   ├── __init__.py
│   ├── datasets.py               # DonutCsvDataset + DonutDataCollator
│   ├── train.py                  # Training entrypoint with HF Trainer
│   └── inference.py              # Single-image inference
└── utils
    ├── __init__.py
    ├── config_manager.py         # load_config + make_run_dirs
    └── logging_utils.py          # unified logger (file + console)
```

## File Roles

- `config.json`: Centralized config for model name, output dir, seed, data paths, and training hyperparams.
- `scripts/divide_data.py`: Reads `data/original_data/image_data.csv` and writes `data/train_data.csv`, `data/validation_data.csv`, `data/test_data.csv` + a small summary JSON.
- `src/datasets.py`: Dataset and collator for pairing images with text tokens; handles padding labels to `-100` for loss masking.
- `src/train.py`: Loads processor/model, datasets, training args; runs training with `Trainer`. Saves model/processor and metadata to a timestamped run directory.
- `src/inference.py`: Loads a saved model/processor and runs greedy generation for a single image.
- `utils/config_manager.py`: Utilities to load `config.json` and create timestamped run folders.
- `utils/logging_utils.py`: Logger that writes to `application.log`, `error.log`, and stdout.

## Configuration

`config.json` fields:

- `model_name`: e.g., "naver-clova-ix/donut-base" (Hugging Face Hub ID).
- `output_root`: root for run outputs, e.g., "results".
- `seed`: global seed for split/training.
- `split`: ratios and options for data splitting.
- `train`: training hyperparameters (epochs, batch sizes, lr, warmup, etc.). Set "fp16": false if training on CPU.
- `data`: CSV paths and `image_root` for resolving relative image paths.

## Data Preparation

- Prepare your CSV at `data/original_data/image_data.csv` with columns:
  - `image_path`: absolute or relative path to the image file.
  - `text`: target text to generate.
- Split into train/valid/test:
  - `python scripts/divide_data.py --config config.json`
  - Outputs:
    - `data/train_data.csv`
    - `data/validation_data.csv`
    - `data/test_data.csv`
    - `data/data_split_summary.json`

## Training

- Start training:
  - `python src/train.py`
- What gets created under a new timestamped run in `results/run_YYYYMMDD_HHMMSS/`:
  - `checkpoints/`: intermediate Hugging Face checkpoints (`checkpoint-XXXX`).
  - `logs/`: `application.log` (INFO) and `error.log` (ERROR).
  - `model/`: final `config.json` (model), tokenizer, processor, and weights saved via `save_pretrained`.
  - `run_metadata.json`: summary with best metric and global step.

Notes:

- If you encounter OOM on GPU, lower `per_device_train_batch_size` or increase `gradient_accumulation_steps` in `config.json`.
- On CPU-only environments, set "fp16": false in `config.json`.

## Inference

- Run single-image inference using a trained model directory:
  - `python src/inference.py --model_dir results/run_YYYYMMDD_HHMMSS/model --image path/to/image.png`
- The script returns the generated sequence after removing EOS/PAD tokens.
- For task-specific prompting, modify the empty prompt in `src/inference.py` where `decoder_input_ids` is created.

## Hugging Face: Download and Save Locations

- Download (from_pretrained):

  - Code paths: `src/train.py` and `src/inference.py` call `DonutProcessor.from_pretrained(model_name_or_dir)` and `VisionEncoderDecoderModel.from_pretrained(model_name_or_dir)`.
  - When given a Hub ID (e.g., `naver-clova-ix/donut-base`), Transformers downloads weights and configs to the local cache.
  - Default cache: `~/.cache/huggingface/transformers` (Linux/macOS) or `%USERPROFILE%\.cache\huggingface\transformers` (Windows).
  - To customize, set environment variables before running:
    - `HF_HOME=/path/to/hf_home` (affects all HF caches)
    - or `TRANSFORMERS_CACHE=/path/to/transformers_cache` (Transformers only)

- Save (save_pretrained):
  - After training, the final model and processor are written to `results/run_YYYYMMDD_HHMMSS/model/` by:
    - `model.save_pretrained(paths.model_dir)` and
    - `processor.save_pretrained(paths.model_dir)` in `src/train.py`.
  - Checkpoints during training are saved under `results/run_YYYYMMDD_HHMMSS/checkpoints/` by the Trainer.
  - Inference loads directly from a local `model/` folder via `--model_dir`.

## Quick Commands

- Split data: `python scripts/divide_data.py --config config.json`
- Train: `python src/train.py`
- Inference: `python src/inference.py --model_dir results/run_YYYYMMDD_HHMMSS/model --image path/to/image.png`

## Troubleshooting

- CUDA toolkit mismatch: ensure your installed PyTorch wheel matches your system CUDA. Use the official selector on pytorch.org.
- Tokenizer pad token issues: the code sets `pad_token_id`, `eos_token_id`, and `decoder_start_token_id` to avoid generation/training warnings.
- Paths in CSV: if `image_path` is relative, it is resolved against `config.json` → `data.image_root`.
